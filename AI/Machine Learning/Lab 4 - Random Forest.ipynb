{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Random Forest\n",
    "\n",
    "In this Lab, you will implement a very simplified version of the Random Forest classifier. Make sure that you check the videos of lecture 4 before starting this Lab:\n",
    "- Decision Tree and Random Forest: https://youtu.be/YSy9S2OsXNo\n",
    "\n",
    "Random Forst (RF) works as follows:\n",
    "\n",
    "- Given a dataset $X \\in \\mathbb{R}^{n \\times d}$, RF selects randomly some features (i.e. `n_features`) from the dataset (where `n_features << d`). It also selects a random subset of the data (with `n_samples` data-points). Then, it builds a Decision Tree from those selected features and samples.\n",
    "\n",
    "- Repeats this process `n_trees` times so that you have a number of `n_trees` Decision Trees built from different random combinations of features and different random subsets of data.\n",
    "\n",
    "- To predict, RF takes each of the `n_trees` built Decision Trees and predict the outputs (classes); then it calculates the votes for each predicted class-label and takes the mode (most frequent label). In other words, considers the high voted predicted label as the final prediction from the random forest algorithm.\n",
    "\n",
    "## Loading the dataset:\n",
    "The code below will load a training dataset into variables `X` and `y` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [34.62365962 78.02469282]       y =  0.0\n",
      "X =  [30.28671077 43.89499752]       y =  0.0\n",
      "X =  [35.84740877 72.90219803]       y =  0.0\n",
      "X =  [60.18259939 86.3085521 ]       y =  1.0\n",
      "X =  [79.03273605 75.34437644]       y =  1.0\n",
      "X =  [45.08327748 56.31637178]       y =  0.0\n",
      "X =  [61.10666454 96.51142588]       y =  1.0\n",
      "X =  [75.02474557 46.55401354]       y =  1.0\n",
      "X =  [76.0987867  87.42056972]       y =  1.0\n",
      "X =  [84.43281996 43.53339331]       y =  1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "\n",
    "filename = \"datasets/university-admission-dataset.csv\"\n",
    "mydata = np.genfromtxt(filename, delimiter=\",\")\n",
    "X = mydata[:, :2]\n",
    "y = mydata[:, -1]\n",
    "\n",
    "\"\"\" TODO:\n",
    "Print a small subset of X and y to see how the data looks like.\n",
    "\"\"\"\n",
    "for i in range(10):\n",
    "    print(\"X = \",X[i],\"      y = \",y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn.tree.DecisionTreeClassifier\n",
    "To simplify the implementation of our Random Forest classifier, we will use an existing implementation of the `DecisionTreeClassifier` available in the sklearn library. Read the following code to see an example of how to use the `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n",
      "Training Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TODO:\n",
    "Read and run the following example code to see how to use a \n",
    "simple DecisionTreeClassifier to make predictions.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0).fit(X, y) # Training\n",
    "preds = clf.predict(X) # Predicting\n",
    "\n",
    "print(\"Predictions:\", preds)\n",
    "\n",
    "acc = np.mean(preds == y) * 100\n",
    "print(\"Training Accuracy: {}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a simplified random forest classifier\n",
    "Complete the code below to implement a random forest classifier.\n",
    "\n",
    "In Python, to select a list of `k` random integers between `0` and `nbr` (excluded), you can use `ids = np.random.choice(nbr, k, replace=False)`. The keyword `replace=False` means that we don't want to select the same number more than once (so, ids with contain `k` unique integers).\n",
    "\n",
    "Also, to compute the mode (most common or frequent label), you can make use of the function `scipy.stats.mode` if you want. Read its documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 94.0%\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Complete the definition of the function train(...) which allows to train an ensemble \n",
    "of decision tree classifiers. Each trained decision tree clf is saved together with \n",
    "its corresponding features indices (used to train clf) into the list clfs.\n",
    "\n",
    "Arguments:\n",
    "*** X: the training dataset.\n",
    "*** y: the training labels corresponding to X.\n",
    "*** n_features: the number of features to select randomly to build a decision tree.\n",
    "*** n_samples: the number of instances to select randomly to build a decision tree.\n",
    "*** n_trees: the number of decision trees to build.\n",
    "\"\"\"\n",
    "def train_(X, y, n_features, n_samples, n_trees):\n",
    "    n, d = X.shape # n: number of rows, and d: number of columns\n",
    "    clfs = [] # used to store the decision trees with their corresponding features indices.\n",
    "    \n",
    "    for itr in range(n_trees):\n",
    "        pass\n",
    "        # TODO: select randomly n_features indices between 0 and d and store them in ids_columns\n",
    "        ids_columns = [np.random.choice(range(0, d)) for x in range(n_features)]\n",
    "        #print(d)\n",
    "        #print(ids_columns)\n",
    "        \n",
    "        # TODO: select randomly n_samples indices between 0 and n and store them in ids_samples\n",
    "        ids_samples = [np.random.choice(range(0, n)) for x in range(n_samples)]\n",
    "        #print(n)\n",
    "        #print(ids_samples)\n",
    "        \n",
    "        # TODO: select the subset of X corresponding to the rows ids_samples and the columns ids_columns\n",
    "        Xsub = X[ids_samples, ids_columns].reshape(50, -1)\n",
    "        #print(Xsub)\n",
    "        \n",
    "        # TODO: select the subset of y corresponding to ids_samples\n",
    "        ysub = y[ids_samples].reshape(50, -1)\n",
    "        #print(ysub)\n",
    "        \n",
    "        # TODO: train a decision tree classifier clf, based on the training subset: Xsub, ysub\n",
    "        clf = DecisionTreeClassifier().fit(Xsub, ysub)\n",
    "        \n",
    "        # TODO: save the trained decision tree classifier (clf) and the corresponding \n",
    "        # feature indices (ids_columns) used to train this classifier, into the list clfs.\n",
    "        clfs.append((clf, ids_columns))\n",
    "        \n",
    "    return clfs\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Complete the definition of the function predict(clfs, X) which allows to predict the class-labels \n",
    "for the instances in the given dataset X, using the ensemble clfs returned by your function previous function.\n",
    "\"\"\"\n",
    "def predict(clfs, X):\n",
    "    y_pred = np.zeros(len(X))\n",
    "    for clf, ids_columns in clfs:\n",
    "        Xsub = X[:, ids_columns]\n",
    "        y_pred_sub = clf.predict(Xsub)\n",
    "        y_pred += y_pred_sub\n",
    "    \n",
    "    y_pred = y_pred / len(clfs)\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\"\"\" TODO:\n",
    "Uncomment the code below to test your functions:\n",
    "\"\"\"\n",
    "clfs = train_(X, y, n_features=1, n_samples=50, n_trees=10)\n",
    "y_pred = predict(clfs, X)\n",
    "\n",
    "acc = np.mean(y_pred == y) * 100\n",
    "print(\"Training Accuracy: {}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Testing your implementation a dataset with multiple features\n",
    "This part is optional.\n",
    "\n",
    "Test you random forest implementation on a dataset with more features (e.g. you can add more polynomial features to the previous dataset or use any other dataset with a high number of features).\n",
    "\n",
    "Compute the generalization accuracy of your random forest on that dataset using a 10-fold-cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (optional) ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
